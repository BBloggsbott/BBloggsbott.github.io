---
title: 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)'
date: 2025-12-27
permalink: /read-a-paper/t5/
tags:
  - machine learning
  - llm
comments: true
description: Dive deep into the 'Attention Is All You Need' paper. This detailed guide deconstructs the Transformer architecture, explaining Self-Attention, Multi-Head Attention, and how it replaced RNNs in NLP.
---

{% include base_path %}

| Original Paper link: [https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683) |
|--|
|PDF with additional annotations: [Download Summary]({{ base_path }}/files/read-a-paper/T5.pdf)|


This paper presents a systematic empirical survey of transfer learning techniques in Natural Language Processing (NLP). Its primary contribution is not a novel architecture, but rather a **unified framework** that treats every text processing problem - classification, translation, summarization, and regression - as a "text-to-text" generation task.

By unifying the input/output format, the authors were able to rigorously compare different model architectures, pre-training objectives, and dataset filtering methods. The study culminates in the release of the **Colossal Clean Crawled Corpus (C4)** and the **T5 model**, which, when scaled to 11 billion parameters, achieved state-of-the-art results on benchmarks including GLUE, SQuAD, and SuperGLUE.

## Core Methodology: The Unified Text-to-Text Framework

Prior to T5, transfer learning was fragmented. Models like BERT were used for classification (outputting class labels), while Seq2Seq models were used for translation. T5 standardizes this by forcing all tasks to accept a text sequence as input and generate a text sequence as output.

### Task Formatting
The model uses a "task prefix" to distinguish between operations. This unified interface allows the same loss function (Cross-Entropy), hyperparameters, and decoding strategy to be used universally.

1. Machine Translation
    * *Input:* `translate English to German: That is good.`
    * *Target:* `Das ist gut.`
1. Classification (e.g., MNLI)
    * *Input:* `mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.`
    * *Target:* `entailment`
    * *Note:* The model does not output a class ID (0/1/2). It is trained to generate the literal string "entailment", "contradiction", or "neutral".
1. Regression (e.g., STS-B)
    * *Input:* `stsb sentence1: The cat sat. sentence2: The cat lay down.`
    * *Target:* `3.8`
    * *Mechanism:* Continuous float values are rounded to the nearest 0.2 increment and converted to string literals (e.g., 2.57 becomes "2.6").

## Data Engineering: The C4 Dataset
To support massive scale, the authors created the **Colossal Clean Crawled Corpus (C4)*, a 750 GB dataset derived from the Common Crawl.

### Cleaning Heuristics
The paper emphasizes that model performance is strictly limited by data quality. Raw web text is noisy, containing code, menus, and error messages. Key filtering heuristics included:

* **Punctuation:** Retaining only lines ending in terminal punctuation (., ?, !, ").
* **Length:** Discarding pages $$<$$ 5 sentences and lines $$<$$ 5 words.
* **Deduplication:** Removing any 3-sentence span that appeared more than once in the dataset to prevent memorization of boilerplate text.
* **Language Detection:** Filtering for English text with $$>$$ 99% probability.
* **Content Safety:** Removal of "List of Dirty, Naughty, Obscene or Otherwise Bad Words".

## Empirical Study: Architecture & Objectives
The authors conducted an extensive ablation study to determine the optimal model structure and pre-training objective.

### Architecture Comparison

Three architectural variants were tested:

1. **Encoder-Decoder (Standard Transformer):** Fully-visible attention in the encoder; causal attention in the decoder.
1. **Language Model (GPT-style):** Decoder-only stack with causal attention.
1. **Prefix LM:** A hybrid decoder-only architecture.
    * **Mechanism:** Uses fully-visible attention over the "input" segment (like BERT) and switches to causal attention for the "target" segment (like GPT).

**Finding:** The **Encoder-Decoder** architecture consistently outperformed the others. Although the Prefix LM was competitive, the explicit separation of reading (encoding) and writing (decoding) proved most effective for the text-to-text format.

### Pre-training Objective
The study compared Language Modeling (predict next word), Deshuffling, and Denoising objectives.
**Finding:** Span-Corruption (Denoising) was optimal.

* **Mechanism:** Instead of masking single tokens (BERT), T5 masks contiguous **spans** of text and replaces them with unique sentinel tokens (`<X>`, `<Y>`).
* **Efficiency:** The target sequence consists *only* of the missing spans, making training computationally cheaper than regenerating the full sentence.
* **Parameters:** A corruption rate of 15\% with an average span length of 3 tokens yielded the best results.

## Training & Scaling Strategies
### Fine-Tuning vs. Parameter Efficiency

The authors investigated methods to adapt the pre-trained model to downstream tasks efficiently.
* **Full Fine-Tuning:** Updating all parameters.
* **Adapter Layers:** Freezing the main model and training small dense-ReLU-dense blocks inserted between layers.

**Finding:** While **Adapter Layers** offer significant memory savings during training, they consistently underperformed compared to **Full Fine-Tuning**. To match performance, adapters had to be scaled up significantly, negating their efficiency benefits.

### Multi-Task Learning & Sampling
To train a single model on multiple tasks simultaneously, data imbalance must be addressed.

* **Strategy:** **Temperature-Scaled Mixing**.
* *Problem:* Large tasks (Translation) drown out small tasks (GLUE) if sampled by size. Equal sampling causes overfitting on small tasks.
* *Solution:* The probability of sampling a task is raised to the power of $$1/T$$.
    * $$T=1$$: Proportional sampling.
    * $$T=\infty$$: Equal sampling.
* **T5 Approach:** Using $T=2$ or similar artificially boosts the sampling rate of low-resource tasks without allowing them to dominate.

### Scaling Laws (The "Bitter Lesson")

The paper posed a resource allocation question: Given a fixed 4x increase in compute, is it better to train longer, use larger batches, or build a larger model?

**Finding:** **Increasing model size** provided the most consistent gains. This finding led to the development of **T5-11B** (11 billion parameters), which achieved SOTA performance simply by virtue of its scale, reinforcing the "Bitter Lesson" that general methods scaling with compute often outperform hand-engineered optimizations.

# Key Results & Conclusion

The final T5-11B model achieved:

* **GLUE:** 90.3 (SOTA).
* **SuperGLUE:** 88.9 (SOTA), nearing human baseline (89.8).
* **SQuAD:** State-of-the-art on Exact Match scores.

**Conclusion:** T5 represents a consolidation of transfer learning knowledge. By simplifying the interface (Text-to-Text) and maximizing the scale of data (C4) and parameters (11B), it established a robust baseline for modern NLP. While newer models like Gemini have shifted toward decoder-only, multimodal architectures, T5 remains a standard for functional, high-precision text generation tasks.
